# Reconhecimento de Gestos com as MÃ£os em Tempo Real
---

## ğŸ“‚ Estrutura do Projeto

Aqui estÃ¡ uma descriÃ§Ã£o de cada arquivo e diretÃ³rio principal do projeto:

-   `ğŸ“ dataset/`: DiretÃ³rio que armazena as imagens de cada gesto utilizadas para o treinamento. Cada subpasta dentro de `dataset` representa uma classe de gesto (ex: `A/`, `B/`, `C/`).
-   `ğŸ“„ fazer_dataset.ipynb`: Um Jupyter Notebook utilizado para coletar as imagens que compÃµem o dataset. Ele auxilia na captura de imagens da webcam e na organizaÃ§Ã£o das mesmas nas pastas corretas.
-   `ğŸ“„ treino.py`: Script Python responsÃ¡vel por processar os dados do `dataset`, treinar o modelo de classificaÃ§Ã£o (usando `RandomForestClassifier`) e salvar o modelo treinado no arquivo `model.p`.
-   `ğŸ“„ inferencia.py`: O script principal da aplicaÃ§Ã£o. Ele carrega o modelo treinado (`model.p`), inicia a webcam e realiza a detecÃ§Ã£o e classificaÃ§Ã£o dos gestos em tempo real, exibindo o resultado na tela.
-   `ğŸ“„ model.p`: Arquivo binÃ¡rio que contÃ©m o modelo de Machine Learning jÃ¡ treinado. Ã‰ gerado pelo `treino.py` e utilizado pelo `inferencia.py`.

---

## ğŸ› ï¸ Tecnologias Utilizadas

-   **Python 3.x**
-   **OpenCV (`opencv-python`)**: Para captura e manipulaÃ§Ã£o de vÃ­deo da webcam.
-   **MediaPipe**: Para a detecÃ§Ã£o dos landmarks da mÃ£o em tempo real.
-   **Scikit-learn**: Para o treinamento do modelo de classificaÃ§Ã£o.
-   **NumPy**: Para manipulaÃ§Ã£o eficiente de arrays numÃ©ricos.
-   **Jupyter Notebook**: Para a etapa interativa de coleta de dados.

---


## Video demonstraÃ§Ã£o
- https://drive.google.com/file/d/1QGOFt2yAIscC2j9ZZX_7zY2wjTsZETwa/view?usp=sharing
